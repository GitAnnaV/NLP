{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oer5fhq2G-k0",
        "outputId": "ba69d9eb-8e90-46ef-a64f-d3bc0846400b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Применить к текстам лемматизацию, удаление стоп слов и токенизацию по словам"
      ],
      "metadata": {
        "id": "thnslyFXVKXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(texts: list[str]) -> list[list[str]]:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('russian'))  # Для русского языка\n",
        "    processed_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "        # Токенизация\n",
        "        tokens = word_tokenize(text.lower(), language='russian')\n",
        "        # Удаление стоп-слов и лемматизация\n",
        "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "        processed_texts.append(processed_tokens)\n",
        "\n",
        "    return processed_texts\n",
        "texts = [\"Задача организации, в особенности же постоянное информационно-техническое обеспечение нашей деятельности требует определения и уточнения системы масштабного изменения ряда параметров. Практический опыт показывает, что начало повседневной работы по формированию позиции представляет собой интересный эксперимент проверки системы обучения кадров, соответствующей насущным потребностям! Задача организации, в особенности же дальнейшее развитие различных форм деятельности напрямую зависит от всесторонне сбалансированных нововведений?\"]\n",
        "processed_texts = preprocessing(texts)\n",
        "print(\"Processed Texts:\", processed_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u812YRB0TNSg",
        "outputId": "29b7099c-58b4-4074-dfa4-41f4b7e9769e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Texts: [['задача', 'организации', 'особенности', 'постоянное', 'обеспечение', 'нашей', 'деятельности', 'требует', 'определения', 'уточнения', 'системы', 'масштабного', 'изменения', 'ряда', 'параметров', 'практический', 'опыт', 'показывает', 'начало', 'повседневной', 'работы', 'формированию', 'позиции', 'представляет', 'собой', 'интересный', 'эксперимент', 'проверки', 'системы', 'обучения', 'кадров', 'соответствующей', 'насущным', 'потребностям', 'задача', 'организации', 'особенности', 'дальнейшее', 'развитие', 'различных', 'форм', 'деятельности', 'напрямую', 'зависит', 'всесторонне', 'сбалансированных', 'нововведений']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Реализовать Bag of Words"
      ],
      "metadata": {
        "id": "TtEkWRFCVYep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def make_dict(texts: list[list[str]]) -> dict[str, int]:\n",
        "    word_dict = defaultdict(int)\n",
        "    for text in texts:\n",
        "        for word in text:\n",
        "            word_dict[word] += 1\n",
        "    return word_dict\n",
        "\n",
        "def count_num_words(texts: list[list[str]]) -> list[int]:\n",
        "    word_counts = []\n",
        "    for text in texts:\n",
        "        word_counts.append(len(text))\n",
        "    return word_counts\n",
        "word_dict = make_dict(processed_texts)\n",
        "print(\"Word Dictionary:\", word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1uXNZPwKqs9",
        "outputId": "91c9a65c-3225-48d8-a155-613d4db9f2f8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Dictionary: defaultdict(<class 'int'>, {'задача': 2, 'организации': 2, 'особенности': 2, 'постоянное': 1, 'обеспечение': 1, 'нашей': 1, 'деятельности': 2, 'требует': 1, 'определения': 1, 'уточнения': 1, 'системы': 2, 'масштабного': 1, 'изменения': 1, 'ряда': 1, 'параметров': 1, 'практический': 1, 'опыт': 1, 'показывает': 1, 'начало': 1, 'повседневной': 1, 'работы': 1, 'формированию': 1, 'позиции': 1, 'представляет': 1, 'собой': 1, 'интересный': 1, 'эксперимент': 1, 'проверки': 1, 'обучения': 1, 'кадров': 1, 'соответствующей': 1, 'насущным': 1, 'потребностям': 1, 'дальнейшее': 1, 'развитие': 1, 'различных': 1, 'форм': 1, 'напрямую': 1, 'зависит': 1, 'всесторонне': 1, 'сбалансированных': 1, 'нововведений': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "b7SC3RcaVdx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfidf(texts: list[list[str]]) -> list[list[float]]:\n",
        "    # Преобразуем список токенов обратно в строки для TfidfVectorizer\n",
        "    texts = [' '.join(text) for text in texts]\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "    return tfidf_matrix.toarray()\n",
        "tfidf_matrix = tfidf(processed_texts)\n",
        "print(\"TF-IDF Matrix:\", tfidf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS_lT2LaK-2g",
        "outputId": "c2583411-0e8d-4464-fe93-107561fec640"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix: [[0.13245324 0.13245324 0.26490647 0.13245324 0.26490647 0.13245324\n",
            "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
            "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
            "  0.26490647 0.26490647 0.13245324 0.13245324 0.13245324 0.13245324\n",
            "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324\n",
            "  0.13245324 0.13245324 0.13245324 0.13245324 0.26490647 0.13245324\n",
            "  0.13245324 0.13245324 0.13245324 0.13245324 0.13245324 0.13245324]]\n"
          ]
        }
      ]
    }
  ]
}